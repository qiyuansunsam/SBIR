{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Activation, LeakyReLU, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Add\n",
    "import math\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_path    = '../datasets/unbraid/img/train'\n",
    "train_sketchs_path = '../datasets/unbraid/sketch/train'\n",
    "val_imgs_path      = '../datasets/unbraid/img/test'\n",
    "val_sketchs_path   = '../datasets/unbraid/sketch/test'\n",
    "\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "channels = 3\n",
    "batch_size = 16 # Adjust as needed\n",
    "input_shape = (img_height, img_width, channels)\n",
    "latent_dim = 32  # Dimension for z_inv, mu, log_var each [cite: 147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../datasets/unbraid/img/train and ../datasets/unbraid/sketch/train...\n",
      " Processing image 500/3000\n",
      " Processing image 1000/3000\n",
      " Processing image 1500/3000\n",
      " Processing image 2000/3000\n",
      " Processing image 2500/3000\n",
      "Data loading complete. Processed 3000 pairs. Skipped 0 due to errors.\n",
      "Loading data from ../datasets/unbraid/img/test and ../datasets/unbraid/sketch/test...\n",
      "Data loading complete. Processed 466 pairs. Skipped 0 due to errors.\n",
      "Training samples: (3000, 128, 128, 3)\n",
      "Validation samples: (466, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_id(filename):\n",
    "    base = os.path.splitext(filename)[0]\n",
    "    # Handle potential multiple underscores if necessary\n",
    "    parts = base.split('_')\n",
    "    if len(parts) > 1:\n",
    "        return parts[-1]\n",
    "    return base # Fallback if no underscore\n",
    "\n",
    "# --- Modified load_data Function ---\n",
    "def load_data(img_path, sketch_path, img_width, img_height): # Added img_width, img_height as args\n",
    "    \"\"\"\n",
    "    Loads image-sketch pairs, normalizes them to [0, 1],\n",
    "    and checks for/handles NaN/inf values.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image directory.\n",
    "        sketch_path (str): Path to the sketch directory.\n",
    "        img_width (int): Target width for resizing.\n",
    "        img_height (int): Target height for resizing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (np.array(images), np.array(sketches), np.array(labels))\n",
    "               Images/sketches are float32 arrays normalized to [0, 1].\n",
    "    \"\"\"\n",
    "    img_files = [f for f in os.listdir(img_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    sketch_files = [f for f in os.listdir(sketch_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    sketch_dict = { get_id(sf): sf for sf in sketch_files }\n",
    "\n",
    "    images, sketches, labels = [], [], []\n",
    "    skipped_count = 0\n",
    "\n",
    "    print(f\"Loading data from {img_path} and {sketch_path}...\")\n",
    "    for i, img_filename in enumerate(img_files):\n",
    "        if i % 500 == 0 and i > 0: # Print progress occasionally\n",
    "             print(f\" Processing image {i}/{len(img_files)}\")\n",
    "\n",
    "        img_id = get_id(img_filename)\n",
    "        if img_id in sketch_dict:\n",
    "            full_img_path = os.path.join(img_path, img_filename)\n",
    "            full_sketch_path = os.path.join(sketch_path, sketch_dict[img_id])\n",
    "\n",
    "            try:\n",
    "                # Load and process sketch\n",
    "                sketch_pil = Image.open(full_sketch_path).convert('RGB')\n",
    "                sketch_pil = sketch_pil.resize((img_width, img_height))\n",
    "                # Normalize to [0, 1]\n",
    "                sketch_arr = np.array(sketch_pil, dtype=np.float32) / 255.0\n",
    "\n",
    "                # Load and process image\n",
    "                img_pil = Image.open(full_img_path).convert('RGB')\n",
    "                img_pil = img_pil.resize((img_width, img_height))\n",
    "                # Normalize to [0, 1]\n",
    "                img_arr = np.array(img_pil, dtype=np.float32) / 255.0\n",
    "\n",
    "                # --- Check for NaN/inf and handle ---\n",
    "                if np.isnan(sketch_arr).any() or np.isinf(sketch_arr).any():\n",
    "                    warnings.warn(f\"NaN or Inf found in sketch: {full_sketch_path}. Replacing with 0.\")\n",
    "                    sketch_arr = np.nan_to_num(sketch_arr, nan=0.0, posinf=0.0, neginf=0.0) # Replace NaN/inf with 0\n",
    "\n",
    "                if np.isnan(img_arr).any() or np.isinf(img_arr).any():\n",
    "                    warnings.warn(f\"NaN or Inf found in image: {full_img_path}. Replacing with 0.\")\n",
    "                    img_arr = np.nan_to_num(img_arr, nan=0.0, posinf=0.0, neginf=0.0) # Replace NaN/inf with 0\n",
    "                # --- End Check ---\n",
    "\n",
    "                images.append(img_arr)\n",
    "                sketches.append(sketch_arr)\n",
    "                labels.append(img_id)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch potential errors during image opening/processing\n",
    "                print(f\"Error processing pair: img={img_filename}, sketch={sketch_dict[img_id]}. Error: {e}\")\n",
    "                skipped_count += 1\n",
    "\n",
    "    print(f\"Data loading complete. Processed {len(labels)} pairs. Skipped {skipped_count} due to errors.\")\n",
    "\n",
    "    if not images: # Handle case where no valid pairs were found\n",
    "        warnings.warn(\"No valid image-sketch pairs found.\")\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    return np.array(images, dtype=np.float32), np.array(sketches, dtype=np.float32), np.array(labels)\n",
    "\n",
    "\n",
    "# Load training and validation data\n",
    "train_images, train_sketches, train_labels = load_data(train_imgs_path, train_sketchs_path, img_width, img_height)\n",
    "val_images, val_sketches, val_labels = load_data(val_imgs_path, val_sketchs_path, img_width, img_height)\n",
    "print(\"Training samples:\", train_images.shape)\n",
    "print(\"Validation samples:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tf.data Datasets...\n",
      "Training and validation datasets created.\n"
     ]
    }
   ],
   "source": [
    "def normalize_to_tanh(image):\n",
    "    return (image * 2.0) - 1.0\n",
    "\n",
    "# --- Data Generator Function ---\n",
    "def triplet_generator(sketches, images, labels):\n",
    "    \"\"\"\n",
    "    Generator function to yield batches of (anchor_sketch, positive_photo, negative_photo)\n",
    "    along with a target image for reconstruction.\n",
    "\n",
    "    Args:\n",
    "        sketches (np.array): Array of sketch images (normalized 0-1).\n",
    "        images (np.array): Array of photo images (normalized 0-1).\n",
    "        labels (np.array): Array of corresponding labels (IDs).\n",
    "    \"\"\"\n",
    "    num_samples = sketches.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "\n",
    "    # Create a mapping from label to list of indices for faster lookup (optional but good for large datasets)\n",
    "    label_to_indices = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label not in label_to_indices:\n",
    "            label_to_indices[label] = []\n",
    "        label_to_indices[label].append(idx)\n",
    "\n",
    "    while True: # Generators used with from_generator should loop indefinitely\n",
    "        # Shuffle indices each epoch (optional, good practice)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            # --- Anchor and Positive ---\n",
    "            anchor_sketch = sketches[i]\n",
    "            positive_photo = images[i] # Direct correspondence from load_data\n",
    "            anchor_label = labels[i]\n",
    "\n",
    "            # --- Negative Sampling ---\n",
    "            # Keep sampling until we find an image with a different label\n",
    "            while True:\n",
    "                # USE random.choice HERE\n",
    "                negative_idx = random.choice(indices) # Now 'random' is defined\n",
    "                if labels[negative_idx] != anchor_label:\n",
    "                    negative_photo = images[negative_idx]\n",
    "                    break\n",
    "\n",
    "            # --- Normalization ---\n",
    "            anchor_sketch = normalize_to_tanh(anchor_sketch)\n",
    "            positive_photo = normalize_to_tanh(positive_photo)\n",
    "            negative_photo = normalize_to_tanh(negative_photo)\n",
    "\n",
    "            # Target for reconstruction is the anchor sketch itself\n",
    "            target_image = anchor_sketch\n",
    "\n",
    "            yield (anchor_sketch, positive_photo, negative_photo), target_image\n",
    "\n",
    "\n",
    "# --- Create tf.data Datasets ---\n",
    "\n",
    "# Assuming train_images, train_sketches, train_labels are loaded\n",
    "# Assuming val_images, val_sketches, val_labels are loaded\n",
    "\n",
    "print(\"Creating tf.data Datasets...\")\n",
    "\n",
    "# Training Dataset\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: triplet_generator(train_sketches, train_images, train_labels),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(img_height, img_width, channels), dtype=tf.float32), # Anchor Sketch\n",
    "         tf.TensorSpec(shape=(img_height, img_width, channels), dtype=tf.float32), # Positive Photo\n",
    "         tf.TensorSpec(shape=(img_height, img_width, channels), dtype=tf.float32)), # Negative Photo\n",
    "        tf.TensorSpec(shape=(img_height, img_width, channels), dtype=tf.float32)  # Target Image\n",
    "    )\n",
    ")\n",
    "# Apply batching and prefetching\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Validation Dataset (using the same generator structure)\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: triplet_generator(val_sketches, val_images, val_labels),\n",
    "     output_signature=(\n",
    "        (tf.TensorSpec(shape=(img_height, img_width, channels), dtype=tf.float32), # Anchor Sketch\n",
    "         tf.TensorSpec(shape=(img_height, img_width, channels), dtype=tf.float32), # Positive Photo\n",
    "         tf.TensorSpec(shape=(img_height, img_width, channels), dtype=tf.float32)), # Negative Photo\n",
    "        tf.TensorSpec(shape=(img_height, img_width, channels), dtype=tf.float32)  # Target Image\n",
    "    )\n",
    ")\n",
    "# Apply batching and prefetching\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Training and validation datasets created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Encoder Summary ---\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inception_v3 (Functional)       (None, 2048)         21802784    encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_invariant (Dense)             (None, 32)           65568       inception_v3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 32)           65568       inception_v3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 32)           65568       inception_v3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 21,999,488\n",
      "Trainable params: 21,965,056\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Encoder ---\n",
    "# Using InceptionV3 as the base, as mentioned in the paper [cite: 145]\n",
    "# We'll remove the top classification layer\n",
    "base_encoder = keras.applications.InceptionV3(\n",
    "    include_top=False,\n",
    "    weights='imagenet', # Or None if you want to train from scratch\n",
    "    input_shape=input_shape,\n",
    "    pooling='avg' # Global average pooling to get a feature vector\n",
    ")\n",
    "\n",
    "# Freeze base encoder layers if using pre-trained weights initially\n",
    "# for layer in base_encoder.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "encoder_input = keras.Input(shape=input_shape, name=\"encoder_input\")\n",
    "x = base_encoder(encoder_input, training=False) # Set training=False if layers are frozen\n",
    "\n",
    "# Projection layers to get invariant part and parameters for variable part\n",
    "z_inv = layers.Dense(latent_dim, name=\"z_invariant\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "# Instantiate the encoder model\n",
    "encoder = keras.Model(encoder_input, [z_inv, z_mean, z_log_var], name=\"encoder\")\n",
    "print(\"--- Encoder Summary ---\")\n",
    "encoder.summary()\n",
    "\n",
    "# --- Reparameterization Trick (Sampling Layer) ---\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding the variable part.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        # Use exp(0.5 * log_var) for stddev\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size: 128x128, Start dim: 8x8\n",
      "Calculated number of Conv2DTranspose layers: 4\n",
      " Adding Upsample Layer 1/4 with 128 filters\n",
      " Adding Upsample Layer 2/4 with 64 filters\n",
      " Adding Upsample Layer 3/4 with 64 filters\n",
      " Adding Upsample Layer 4/4 with 32 filters\n",
      "\n",
      "--- Decoder Summary (Input: 32, Output: 128x128x3) ---\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "latent_input (InputLayer)    [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8192)              270336    \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DT (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_584 (Bat (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_20 (ReLU)              (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DT (None, 32, 32, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_585 (Bat (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_21 (ReLU)              (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DT (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_586 (Bat (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_22 (ReLU)              (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_28 (Conv2DT (None, 128, 128, 32)      18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_587 (Bat (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_23 (ReLU)              (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DT (None, 128, 128, 3)       867       \n",
      "=================================================================\n",
      "Total params: 549,123\n",
      "Trainable params: 548,547\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,), name=\"latent_input\")\n",
    "\n",
    "# 2. Define Starting Spatial Dimension and Initial Channels\n",
    "start_dim = 8  # Starting spatial dimension (e.g., 8x8)\n",
    "# Ensure target dimensions are powers of 2 and larger than start_dim\n",
    "if not (img_height == img_width and math.log2(img_height).is_integer() and img_height >= start_dim):\n",
    "    raise ValueError(f\"img_height ({img_height}) and img_width ({img_width}) must be equal,\"\n",
    "                     f\" powers of 2, and >= start_dim ({start_dim}) for this structure.\")\n",
    "\n",
    "initial_channels = 128 # Number of channels after the initial dense layer\n",
    "\n",
    "# 3. Initial Dense and Reshape layers\n",
    "initial_dense_units = start_dim * start_dim * initial_channels\n",
    "x = layers.Dense(initial_dense_units, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((start_dim, start_dim, initial_channels))(x)\n",
    "\n",
    "# 4. Calculate Number of Upsampling Layers needed\n",
    "num_upsample_layers = int(np.log2(img_height / start_dim))\n",
    "print(f\"Target size: {img_height}x{img_height}, Start dim: {start_dim}x{start_dim}\")\n",
    "print(f\"Calculated number of Conv2DTranspose layers: {num_upsample_layers}\")\n",
    "\n",
    "# 5. Build Upsampling Layers Dynamically\n",
    "decoder_filters = [128, 64, 64, 32] # Example filter progression for 128x128 target\n",
    "\n",
    "if len(decoder_filters) != num_upsample_layers:\n",
    "     raise ValueError(f\"Length of decoder_filters ({len(decoder_filters)}) \"\n",
    "                      f\"must match num_upsample_layers ({num_upsample_layers})\")\n",
    "\n",
    "current_channels = initial_channels\n",
    "for i in range(num_upsample_layers):\n",
    "    layer_filters = decoder_filters[i]\n",
    "    print(f\" Adding Upsample Layer {i+1}/{num_upsample_layers} with {layer_filters} filters\")\n",
    "    x = layers.Conv2DTranspose(layer_filters, 3, strides=2, padding=\"same\")(x)\n",
    "    # Using default epsilon for BatchNorm here\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "# 6. Final layer\n",
    "final_activation = \"tanh\"\n",
    "decoder_outputs = layers.Conv2DTranspose(\n",
    "    channels, 3, activation=final_activation, padding=\"same\"\n",
    ")(x)\n",
    "\n",
    "# 7. Instantiate the decoder model\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "print(f\"\\n--- Decoder Summary (Input: {latent_dim}, Output: {img_height}x{img_width}x{channels}) ---\")\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sampling = Sampling()\n",
    "        # Use lists to store metrics for train and val separation if needed,\n",
    "        # or rely on Keras naming convention (val_...)\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.triplet_loss_tracker = keras.metrics.Mean(name=\"triplet_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # These are the metrics reset at the start of each epoch\n",
    "        # Keras automatically adds 'val_' prefix during validation\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.triplet_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\" Forward pass for inference \"\"\"\n",
    "        z_inv, z_mean, z_log_var = self.encoder(inputs)\n",
    "        z_var = self.sampling([z_mean, z_log_var])\n",
    "        # Element-wise sum as per paper [cite: 97]\n",
    "        z_combined = z_inv + z_var\n",
    "        reconstruction = self.decoder(z_combined)\n",
    "        return reconstruction\n",
    "      \n",
    "    def train_step(self, data):\n",
    "        # Assumes data is ((anchor_sketches, positive_photos, negative_photos), target_images)\n",
    "        (anchor_sketches, positive_photos, negative_photos), target_images = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # --- Encoder ---\n",
    "            z_inv_anchor, z_mean_anchor, z_log_var_anchor = self.encoder(anchor_sketches)\n",
    "            z_var_anchor = self.sampling([z_mean_anchor, z_log_var_anchor])\n",
    "            z_combined_anchor = z_inv_anchor + z_var_anchor\n",
    "\n",
    "            # --- Decoder ---\n",
    "            reconstruction = self.decoder(z_combined_anchor)\n",
    "\n",
    "            # --- Reconstruction Loss (Mean) ---\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                 keras.losses.mean_squared_error(target_images, reconstruction)\n",
    "            )\n",
    "\n",
    "            # --- KL Loss ---\n",
    "            exp_log_var = tf.exp(z_log_var_anchor)\n",
    "            kl_loss = -0.5 * (1 + z_log_var_anchor - tf.square(z_mean_anchor) - exp_log_var)\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "\n",
    "            # --- Triplet Loss ---\n",
    "            z_inv_pos, z_mean_pos, z_log_var_pos = self.encoder(positive_photos)\n",
    "            z_inv_neg, z_mean_neg, z_log_var_neg = self.encoder(negative_photos)\n",
    "            z_var_pos = self.sampling([z_mean_pos, z_log_var_pos])\n",
    "            z_combined_pos = z_inv_pos + z_var_pos\n",
    "            z_var_neg = self.sampling([z_mean_neg, z_log_var_neg])\n",
    "            z_combined_neg = z_inv_neg + z_var_neg\n",
    "            triplet_loss = calculate_triplet_loss(\n",
    "                z_inv_anchor, z_inv_pos, z_inv_neg,\n",
    "                z_combined_anchor, z_combined_pos, z_combined_neg\n",
    "            )\n",
    "\n",
    "            # --- Total Loss ---\n",
    "            lambda_1 = 0.001 # KL weight\n",
    "            lambda_2 = 1.0   # Triplet weight\n",
    "            # Ensure losses are finite before combining (optional safeguard)\n",
    "            total_loss = reconstruction_loss + lambda_1 * kl_loss + lambda_2 * triplet_loss\n",
    "\n",
    "        # --- Gradients & Updates ---\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        # --- Update Metrics ---\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.triplet_loss_tracker.update_state(triplet_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    # --- ADD THIS METHOD ---\n",
    "    def test_step(self, data):\n",
    "        # Handles data from validation_data generator\n",
    "        (anchor_sketches, positive_photos, negative_photos), target_images = data\n",
    "\n",
    "        # --- Forward pass for evaluation ---\n",
    "        z_inv_anchor, z_mean_anchor, z_log_var_anchor = self.encoder(anchor_sketches, training=False)\n",
    "        z_var_anchor = self.sampling([z_mean_anchor, z_log_var_anchor])\n",
    "        z_combined_anchor = z_inv_anchor + z_var_anchor\n",
    "        reconstruction = self.decoder(z_combined_anchor, training=False)\n",
    "\n",
    "        # --- Calculate validation losses ---\n",
    "        reconstruction_loss_val = tf.reduce_mean(\n",
    "            keras.losses.mean_squared_error(target_images, reconstruction)\n",
    "        )\n",
    "\n",
    "        exp_log_var_val = tf.exp(z_log_var_anchor)\n",
    "        kl_loss_val = -0.5 * (1 + z_log_var_anchor - tf.square(z_mean_anchor) - exp_log_var_val)\n",
    "        kl_loss_val = tf.reduce_mean(tf.reduce_sum(kl_loss_val, axis=1))\n",
    "\n",
    "        # --- Calculate Triplet Loss for Validation ---\n",
    "        # Encode positive and negative photos\n",
    "        z_inv_pos, z_mean_pos, z_log_var_pos = self.encoder(positive_photos, training=False) # Use training=False\n",
    "        z_inv_neg, z_mean_neg, z_log_var_neg = self.encoder(negative_photos, training=False) # Use training=False\n",
    "        # Sample and combine for z_f\n",
    "        z_var_pos = self.sampling([z_mean_pos, z_log_var_pos])\n",
    "        z_combined_pos = z_inv_pos + z_var_pos\n",
    "        z_var_neg = self.sampling([z_mean_neg, z_log_var_neg])\n",
    "        z_combined_neg = z_inv_neg + z_var_neg\n",
    "        # Calculate actual triplet loss\n",
    "        triplet_loss_val = calculate_triplet_loss(\n",
    "            z_inv_anchor, z_inv_pos, z_inv_neg,\n",
    "            z_combined_anchor, z_combined_pos, z_combined_neg\n",
    "            # Ensure margins here match training margins if desired for comparison\n",
    "        )\n",
    "        # --- End Triplet Calculation ---\n",
    "\n",
    "        # Total validation loss\n",
    "        lambda_1 = 0.001 # Use same weights as training\n",
    "        lambda_2 = 1.0\n",
    "        total_loss_val = reconstruction_loss_val + lambda_1 * kl_loss_val + lambda_2 * triplet_loss_val\n",
    "\n",
    "        # Update Metrics\n",
    "        self.total_loss_tracker.update_state(total_loss_val)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss_val)\n",
    "        self.kl_loss_tracker.update_state(kl_loss_val)\n",
    "        self.triplet_loss_tracker.update_state(triplet_loss_val) # Now tracks actual val triplet loss\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005)) # LR from paper [cite: 154]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " VAE model created and compiled.\n",
      " Note: Triplet loss calculation needs to be implemented and integrated into train_step.\n",
      "       Meta-learning components (FT Layers, Regulariser) are not included here.\n"
     ]
    }
   ],
   "source": [
    "# --- Instantiate the VAE ---\n",
    "\n",
    "def calculate_triplet_loss(anchor_inv, positive_inv, negative_inv,\n",
    "                           anchor_f, positive_f, negative_f,\n",
    "                           margin_inv=0.5, margin_f=0.3):\n",
    "    \"\"\"\n",
    "    Calculates the triplet loss for both invariant (z_inv) and\n",
    "    combined (z_f) features.\n",
    "\n",
    "    Args:\n",
    "        anchor_inv, positive_inv, negative_inv: Batch of invariant features.\n",
    "        anchor_f, positive_f, negative_f: Batch of combined features.\n",
    "        margin_inv: Margin for the invariant feature triplet loss.\n",
    "        margin_f: Margin for the combined feature triplet loss.\n",
    "\n",
    "    Returns:\n",
    "        Total triplet loss for the batch.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are tensors\n",
    "    anchor_inv = tf.convert_to_tensor(anchor_inv, dtype=tf.float32)\n",
    "    positive_inv = tf.convert_to_tensor(positive_inv, dtype=tf.float32)\n",
    "    negative_inv = tf.convert_to_tensor(negative_inv, dtype=tf.float32)\n",
    "    anchor_f = tf.convert_to_tensor(anchor_f, dtype=tf.float32)\n",
    "    positive_f = tf.convert_to_tensor(positive_f, dtype=tf.float32)\n",
    "    negative_f = tf.convert_to_tensor(negative_f, dtype=tf.float32)\n",
    "\n",
    "    # Calculate squared Euclidean distances for invariant features\n",
    "    dist_ap_inv = tf.reduce_sum(tf.square(anchor_inv - positive_inv), axis=-1)\n",
    "    dist_an_inv = tf.reduce_sum(tf.square(anchor_inv - negative_inv), axis=-1)\n",
    "\n",
    "    # Calculate triplet loss for invariant features\n",
    "    loss_inv = tf.maximum(0.0, margin_inv + dist_ap_inv - dist_an_inv)\n",
    "    loss_inv = tf.reduce_mean(loss_inv) # Average over the batch\n",
    "\n",
    "    # Calculate squared Euclidean distances for combined features\n",
    "    dist_ap_f = tf.reduce_sum(tf.square(anchor_f - positive_f), axis=-1)\n",
    "    dist_an_f = tf.reduce_sum(tf.square(anchor_f - negative_f), axis=-1)\n",
    "\n",
    "    # Calculate triplet loss for combined features\n",
    "    loss_f = tf.maximum(0.0, margin_f + dist_ap_f - dist_an_f)\n",
    "    loss_f = tf.reduce_mean(loss_f) # Average over the batch\n",
    "\n",
    "    # Total triplet loss\n",
    "    total_triplet_loss = loss_inv + loss_f\n",
    "    return total_triplet_loss\n",
    "\n",
    "print(\"\\n VAE model created and compiled.\")\n",
    "print(\" Note: Triplet loss calculation needs to be implemented and integrated into train_step.\")\n",
    "print(\"       Meta-learning components (FT Layers, Regulariser) are not included here.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Steps per epoch: 188\n",
      "  Validation steps: 30\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - 40s 156ms/step - total_loss: 2.2347 - reconstruction_loss: 0.0963 - kl_loss: 86.9307 - triplet_loss: 2.0515 - val_total_loss: 0.7889 - val_reconstruction_loss: 0.0288 - val_kl_loss: 94.1624 - val_triplet_loss: 0.6660\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 27s 143ms/step - total_loss: 0.8526 - reconstruction_loss: 0.0352 - kl_loss: 80.7186 - triplet_loss: 0.7367 - val_total_loss: 0.8500 - val_reconstruction_loss: 0.0286 - val_kl_loss: 74.0971 - val_triplet_loss: 0.7473\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 27s 141ms/step - total_loss: 0.8593 - reconstruction_loss: 0.0351 - kl_loss: 77.5180 - triplet_loss: 0.7466 - val_total_loss: 0.8150 - val_reconstruction_loss: 0.0287 - val_kl_loss: 78.4585 - val_triplet_loss: 0.7078\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 27s 141ms/step - total_loss: 0.8964 - reconstruction_loss: 0.0352 - kl_loss: 85.3318 - triplet_loss: 0.7759 - val_total_loss: 0.9212 - val_reconstruction_loss: 0.0287 - val_kl_loss: 83.3771 - val_triplet_loss: 0.8091\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 27s 142ms/step - total_loss: 0.8908 - reconstruction_loss: 0.0351 - kl_loss: 80.7644 - triplet_loss: 0.7749 - val_total_loss: 0.8979 - val_reconstruction_loss: 0.0287 - val_kl_loss: 73.0353 - val_triplet_loss: 0.7962\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 26s 137ms/step - total_loss: 0.8369 - reconstruction_loss: 0.0352 - kl_loss: 77.1268 - triplet_loss: 0.7246 - val_total_loss: 0.7112 - val_reconstruction_loss: 0.0286 - val_kl_loss: 69.4092 - val_triplet_loss: 0.6133\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 25s 134ms/step - total_loss: 0.8471 - reconstruction_loss: 0.0352 - kl_loss: 78.5654 - triplet_loss: 0.7333 - val_total_loss: 0.7452 - val_reconstruction_loss: 0.0286 - val_kl_loss: 71.0198 - val_triplet_loss: 0.6456\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 26s 139ms/step - total_loss: 0.8272 - reconstruction_loss: 0.0328 - kl_loss: 75.3097 - triplet_loss: 0.7191 - val_total_loss: 0.7213 - val_reconstruction_loss: 0.0268 - val_kl_loss: 74.5109 - val_triplet_loss: 0.6199\n",
      "Epoch 9/20\n",
      "171/188 [==========================>...] - ETA: 2s - total_loss: 0.8107 - reconstruction_loss: 0.0318 - kl_loss: 74.8740 - triplet_loss: 0.7041"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Use math.ceil to include the last partial batch\n",
    "    steps_per_epoch = math.ceil(len(train_sketches) / batch_size)\n",
    "    validation_steps = math.ceil(len(val_sketches) / batch_size)\n",
    "    print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"  Validation steps: {validation_steps}\")\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    epochs = 20 # Adjust as needed, maybe start lower (e.g., 5) to check stability first\n",
    "    history = vae.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,     # Use calculated value\n",
    "        validation_steps=validation_steps    # Use calculated value\n",
    "        # Add callbacks here if needed (e.g., ModelCheckpoint, EarlyStopping)\n",
    "        # callbacks=[...]\n",
    "    )\n",
    "    print(\"\\nTraining complete.\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Error: A required variable is not defined before calculating steps or starting training.\")\n",
    "    print(f\"       Please ensure train_sketches, val_sketches, and batch_size are defined.\")\n",
    "    print(f\"       Original error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "\n",
    "# You can now analyze the 'history' object\n",
    "# print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n",
      "Normalizing gallery photos for evaluation...\n",
      "Extracting features for 466 gallery photos...\n",
      "Normalizing test sketches for evaluation...\n",
      "Extracting features for 466 test sketches...\n",
      "Calculating distances...\n",
      "Ranking and calculating Hit@k...\n",
      "  Processing query 100/466\n",
      "  Processing query 200/466\n",
      "  Processing query 300/466\n",
      "  Processing query 400/466\n",
      "Calculation complete.\n",
      "\n",
      "--- Evaluation Results ---\n",
      "Hit@1: 1.50%\n",
      "Hit@5: 4.94%\n",
      "Hit@10: 8.37%\n"
     ]
    }
   ],
   "source": [
    "def normalize_to_tanh(image):\n",
    "    # Assumes input image is in [0, 1] range from load_data\n",
    "    return (image * 2.0) - 1.0\n",
    "\n",
    "# --- Revised Evaluation Function (No tqdm) ---\n",
    "def evaluate_hit_at_k(encoder_model,\n",
    "                       test_sketches_arr,   # NumPy array of test sketches\n",
    "                       test_labels_sketch,  # NumPy array of test sketch labels\n",
    "                       gallery_photos_arr,  # NumPy array of gallery photos\n",
    "                       gallery_labels_photo,# NumPy array of gallery photo labels\n",
    "                       k_values=[1, 5, 10],\n",
    "                       batch_size_eval=32): # Optional batching for prediction\n",
    "    print(\"Normalizing gallery photos for evaluation...\")\n",
    "    gallery_photos_norm = np.array([normalize_to_tanh(img) for img in gallery_photos_arr])\n",
    "\n",
    "    print(f\"Extracting features for {len(gallery_photos_norm)} gallery photos...\")\n",
    "    gallery_features = encoder_model.predict(gallery_photos_norm,\n",
    "                                             batch_size=batch_size_eval,\n",
    "                                             verbose=0)[0] # Index 0 corresponds to z_inv\n",
    "\n",
    "\n",
    "    print(\"Normalizing test sketches for evaluation...\")\n",
    "    test_sketches_norm = np.array([normalize_to_tanh(img) for img in test_sketches_arr])\n",
    "\n",
    "    print(f\"Extracting features for {len(test_sketches_norm)} test sketches...\")\n",
    "    query_features_inv = encoder_model.predict(test_sketches_norm,\n",
    "                                               batch_size=batch_size_eval,\n",
    "                                               verbose=0)[0] # Index 0 corresponds to z_inv\n",
    "\n",
    "    print(\"Calculating distances...\")\n",
    "    all_distances = cdist(query_features_inv, gallery_features, metric='sqeuclidean')\n",
    "\n",
    "    print(\"Ranking and calculating Hit@k...\")\n",
    "    hits = {k: 0 for k in k_values}\n",
    "    num_queries = len(test_sketches_norm)\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        # Optional: Print progress manually if desired\n",
    "        if (i + 1) % 100 == 0: # Print every 100 queries\n",
    "            print(f\"  Processing query {i+1}/{num_queries}\")\n",
    "\n",
    "        true_label = test_labels_sketch[i]\n",
    "        distances = all_distances[i]\n",
    "        ranked_indices = np.argsort(distances)\n",
    "\n",
    "        for k in k_values:\n",
    "            current_k = min(k, len(gallery_labels_photo))\n",
    "            top_k_indices = ranked_indices[:current_k]\n",
    "            top_k_labels = gallery_labels_photo[top_k_indices]\n",
    "            if true_label in top_k_labels:\n",
    "                hits[k] += 1\n",
    "    # --- End change ---\n",
    "\n",
    "    print(\"Calculation complete.\") # Added completion message\n",
    "\n",
    "    hit_results = {f\"Hit@{k}\": (hits[k] / num_queries) * 100.0 for k in k_values}\n",
    "    return hit_results\n",
    "\n",
    "# --- Example Usage (remains the same) ---\n",
    "\n",
    "print(\"\\nRunning evaluation...\")\n",
    "results = evaluate_hit_at_k(\n",
    "    encoder,\n",
    "    val_sketches, # Use loaded NumPy arrays\n",
    "    val_labels,\n",
    "    val_images,\n",
    "    val_labels,\n",
    "    k_values=[1, 5, 10],\n",
    "    batch_size_eval=64\n",
    " )\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "if results:\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value:.2f}%\")\n",
    "else:\n",
    "    print(\"Evaluation did not produce results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
