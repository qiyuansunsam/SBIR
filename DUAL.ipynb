{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras_preprocessing\\image\\affine_transformations.py:13: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.21.6)\n",
      "  import scipy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Activation, LeakyReLU, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Add # Ensure all layers are imported\n",
    "import math # Needed for ceiling division if desired, or use // for floor\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: (3000, 224, 224, 3)\n",
      "Validation samples: (466, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "train_imgs_path    = '../datasets/unbraid/img/train'\n",
    "train_sketchs_path = '../datasets/unbraid/sketch/train'\n",
    "val_imgs_path      = '../datasets/unbraid/img/test'\n",
    "val_sketchs_path   = '../datasets/unbraid/sketch/test'\n",
    "\n",
    "img_height, img_width = 224, 224  # resize shape\n",
    "channels = 3  # use 3 for RGB, or 1 for grayscale\n",
    "\n",
    "def get_id(filename):\n",
    "    base = os.path.splitext(filename)[0]\n",
    "    return base.split('_')[-1]\n",
    "\n",
    "def load_data(img_path, sketch_path):\n",
    "    img_files = [f for f in os.listdir(img_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    sketch_files = [f for f in os.listdir(sketch_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    sketch_dict = { get_id(sf): sf for sf in sketch_files }\n",
    "    \n",
    "    images, sketches, labels = [], [], []\n",
    "    \n",
    "    for img_filename in img_files:\n",
    "        img_id = get_id(img_filename)\n",
    "        if img_id in sketch_dict:\n",
    "            full_img_path = os.path.join(img_path, img_filename)\n",
    "            full_sketch_path = os.path.join(sketch_path, sketch_dict[img_id])\n",
    "            \n",
    "            sketch_pil = Image.open(full_sketch_path).convert('RGB')  # 'L' for grayscale\n",
    "            sketch_pil = sketch_pil.resize((img_width, img_height))\n",
    "            sketch_arr = np.array(sketch_pil, dtype=np.float32) / 255.0\n",
    "\n",
    "            img_pil = Image.open(full_img_path).convert('RGB')\n",
    "            img_pil = img_pil.resize((img_width, img_height))\n",
    "            img_arr = np.array(img_pil, dtype=np.float32) / 255.0\n",
    "            \n",
    "            images.append(img_arr)\n",
    "            sketches.append(sketch_arr)\n",
    "            labels.append(img_id)\n",
    "    \n",
    "    return np.array(images, dtype=np.float32), np.array(sketches, dtype=np.float32), np.array(labels)\n",
    "\n",
    "# Load training and validation data\n",
    "train_images, train_sketches, train_labels = load_data(train_imgs_path, train_sketchs_path)\n",
    "val_images, val_sketches, val_labels = load_data(val_imgs_path, val_sketchs_path)\n",
    "print(\"Training samples:\", train_images.shape)\n",
    "print(\"Validation samples:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_tensor, ratio=16):\n",
    "    \"\"\"Creates a Squeeze-and-Excitation block.\"\"\"\n",
    "    channel_axis = -1 # Assuming channels_last format\n",
    "    filters = input_tensor.shape[channel_axis]\n",
    "    # Handle cases where filters might be None during model building\n",
    "    if filters is None:\n",
    "        # Cannot apply SE block if channel dimension is unknown\n",
    "        # In a functional model, this usually resolves, but as a fallback:\n",
    "        return input_tensor\n",
    "    # Ensure intermediate filters are at least 1\n",
    "    intermediate_filters = max(1, filters // ratio)\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    # Use intermediate_filters calculation\n",
    "    se = Dense(intermediate_filters, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    # Excitation: Multiply the original input tensor by the learned scaling factors\n",
    "    x = multiply([input_tensor, se])\n",
    "    return x\n",
    "\n",
    "# --- Embedding Network with Filters Scaled by Embedding Dimension ---\n",
    "def embedding_net_scaled(input_shape=(64, 64, 1), embedding_dim=128, dropout_rate=0.3, min_base_filters=32, model_name=\"scaled_cnn_encoder\"):\n",
    "    \"\"\"\n",
    "    Creates a CNN encoder where the number of filters in Conv layers\n",
    "    scales based on the embedding_dim.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # --- Calculate Filter Sizes based on embedding_dim ---\n",
    "    # Set base filters relative to embedding_dim, but ensure a minimum width\n",
    "    # Example strategy: Base filters = embedding_dim / 4, minimum 32\n",
    "    base_filters = max(min_base_filters, embedding_dim // 4)\n",
    "\n",
    "    filters_0 = base_filters       # e.g., if embedding_dim=128, filters_0=32. If embedding_dim=256, filters_0=64.\n",
    "    filters_1 = base_filters * 2   # e.g., 64 / 128\n",
    "    filters_2 = base_filters * 4   # e.g., 128 / 256\n",
    "\n",
    "    print(f\"Using filter structure based on embedding_dim={embedding_dim}: {filters_0} -> {filters_1} -> {filters_2}\")\n",
    "\n",
    "    # --- Initial convolution block ---\n",
    "    x = Conv2D(filters_0, (3,3), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Conv2D(filters_0, (3,3), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D((2,2))(x)  # Output shape: (32, 32, filters_0)\n",
    "\n",
    "    # --- Residual block 1 with SE ---\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters_1, (3,3), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Conv2D(filters_1, (3,3), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Add SE block here\n",
    "    x = se_block(x, ratio=16) # Add Squeeze-and-Excitation\n",
    "\n",
    "    # Adjust shortcut filters\n",
    "    shortcut = Conv2D(filters_1, (1,1), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(shortcut)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D((2,2))(x)  # Output shape: (16, 16, filters_1)\n",
    "\n",
    "    # --- Residual block 2 with SE ---\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters_2, (3,3), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Conv2D(filters_2, (3,3), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Add SE block here\n",
    "    x = se_block(x, ratio=16) # Add Squeeze-and-Excitation\n",
    "\n",
    "    # Adjust shortcut filters\n",
    "    shortcut = Conv2D(filters_2, (1,1), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(shortcut)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D((2,2))(x)  # Output shape: (8, 8, filters_2)\n",
    "\n",
    "    # --- Pooling and Embedding Head ---\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x) # Add Dropout before final dense layer\n",
    "    # The final dense layer projects features down to the desired embedding_dim\n",
    "    x = Dense(embedding_dim, use_bias=False, kernel_initializer='he_normal')(x)\n",
    "    outputs = tf.math.l2_normalize(x, axis=1) # L2 normalize the final embedding\n",
    "\n",
    "    return models.Model(inputs, outputs, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using filter structure based on embedding_dim=32: 32 -> 64 -> 128\n",
      "Using filter structure based on embedding_dim=32: 32 -> 64 -> 128\n",
      "Using filter structure based on embedding_dim=32: 32 -> 64 -> 128\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 32 # <--- Change this to see filter sizes adapt\n",
    "dropout_rate_for_encoder = 0.3\n",
    "img_shape = (img_height, img_width, channels)\n",
    "sketch_shape = (img_height, img_width, channels)\n",
    "\n",
    "img_input = Input(img_shape, name=\"image_input\")\n",
    "sketch_input = Input(sketch_shape, name=\"sketch_input\")\n",
    "\n",
    "# Instantiate the SCALED encoders\n",
    "# Pass the desired embedding_dim here\n",
    "image_encoder = embedding_net_scaled(\n",
    "    img_shape,\n",
    "    embedding_dim=embedding_dim, # Pass the dimension\n",
    "    dropout_rate=dropout_rate_for_encoder,\n",
    "    model_name='scaled_image_encoder'\n",
    ")\n",
    "sketch_encoder = embedding_net_scaled(\n",
    "    sketch_shape,\n",
    "    embedding_dim=embedding_dim, # Pass the dimension\n",
    "    dropout_rate=dropout_rate_for_encoder,\n",
    "    model_name='scaled_sketch_encoder'\n",
    ")\n",
    "#img_emb, sketch_emb = image_encoder(img_input), sketch_encoder(sketch_input)\n",
    "\n",
    "shared_encoder = embedding_net_scaled( # Or improved_embedding_network, or your original one\n",
    "    input_shape=img_shape,             # Use the common shape\n",
    "    embedding_dim=embedding_dim,\n",
    "    dropout_rate=dropout_rate_for_encoder,\n",
    "    model_name='shared_encoder'        # Give it a new name\n",
    ")\n",
    "\n",
    "img_emb, sketch_emb = shared_encoder(img_input), shared_encoder(sketch_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Siamese Training Model Summary:\n",
      "Model: \"siamese_trainer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sketch_input (InputLayer)       [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_encoder (Functional)     (None, 32)           306016      image_input[0][0]                \n",
      "                                                                 sketch_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "combined_embedding (Concatenate (None, 64)           0           shared_encoder[0][0]             \n",
      "                                                                 shared_encoder[1][0]             \n",
      "==================================================================================================\n",
      "Total params: 306,016\n",
      "Trainable params: 304,736\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/20\n",
      "375/375 [==============================] - 25s 56ms/step - loss: 0.1276 - val_loss: 0.0279\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0464 - val_loss: 0.0148\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 20s 53ms/step - loss: 0.0297 - val_loss: 0.0097\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0217 - val_loss: 0.0078\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0169 - val_loss: 0.0050\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0135 - val_loss: 0.0036\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0112 - val_loss: 0.0027\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0091 - val_loss: 0.0026\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0076 - val_loss: 0.0014\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0063 - val_loss: 0.0016\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 21s 55ms/step - loss: 0.0053 - val_loss: 0.0011\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0044 - val_loss: 8.3312e-04\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0036 - val_loss: 8.1670e-04\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0029 - val_loss: 4.9084e-04\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0024 - val_loss: 4.9045e-04\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0018 - val_loss: 3.2642e-04\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0014 - val_loss: 2.8536e-04\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.0010 - val_loss: 2.0546e-04\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 7.5243e-04 - val_loss: 1.2979e-04\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 5.2812e-04 - val_loss: 9.8014e-05\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "\"\"# Define your cosine similarity loss (as before)\n",
    "def cosine_similarity_loss(y_true, y_pred):\n",
    "    img_emb_loss = y_pred[:, :embedding_dim]\n",
    "    sketch_emb_loss = y_pred[:, embedding_dim:]\n",
    "    cosine_sim = tf.reduce_sum(img_emb_loss * sketch_emb_loss, axis=1)\n",
    "    return 1.0 - cosine_sim\n",
    "\n",
    "combined_output = tf.keras.layers.concatenate([img_emb, sketch_emb], name=\"combined_embedding\")\n",
    "\n",
    "siamese_model = Model([img_input, sketch_input], combined_output, name=\"siamese_trainer\")\n",
    "\n",
    "# Consider adjusting learning rate, maybe using a scheduler or different optimizer\n",
    "siamese_model.compile(optimizer=Adam(1e-4), loss=cosine_similarity_loss)\n",
    "\n",
    "print(\"Improved Siamese Training Model Summary:\")\n",
    "siamese_model.summary()\n",
    "# Dummy labels for compatibility with Keras fit()\n",
    "dummy_labels_train = np.zeros((train_images.shape[0], 1)) # Correct shape\n",
    "dummy_labels_val = np.zeros((val_images.shape[0], 1))     # Correct shapez\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "siamese_model.fit(\n",
    "    [train_images, train_sketches],\n",
    "    dummy_labels_train,\n",
    "    epochs=20, # Or your desired epochs\n",
    "    batch_size=8, # <--- TRY REDUCING THIS VALUE\n",
    "    validation_data=([val_images, val_sketches], dummy_labels_val)\n",
    ")\n",
    "print(\"--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Retrieval Database ---\n",
      "Generated 466 image embeddings in 1.42 seconds.\n",
      "Generated 466 sketch embeddings in 0.48 seconds.\n",
      "\n",
      "--- Performing Retrieval Analysis ---\n",
      "Calculated similarity matrix (466x466) in 0.00 seconds.\n",
      "\n",
      "--- Retrieval Performance ---\n",
      "Hit@1: 0.00%\n",
      "Hit@5: 1.50%\n",
      "Hit@10: 2.36%\n",
      "\n",
      "--- Example Retrieval for First Sketch ---\n",
      "Query Sketch Index: 0\n",
      "Ground Truth Image Index: 0\n",
      "Top 10 Retrieved Image Indices: [453 419  81 415 328 436 461 355  12 366]\n",
      "Similarities of Top 10: [0.99994576 0.9999439  0.99993986 0.99993825 0.9999382  0.9999362\n",
      " 0.9999359  0.9999355  0.9999341  0.9999323 ]\n",
      "Correct image not found in the top 10.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Building Retrieval Database ---\")\n",
    "# 1. Generate Embeddings for the Image Database (using validation images here)\n",
    "# Ensure encoders are not in training mode if using layers like BatchNormalization differently\n",
    "# (though predict handles this automatically)\n",
    "start_time = time.time()\n",
    "image_embeddings = shared_encoder.predict(val_images, batch_size=32)\n",
    "end_time = time.time()\n",
    "print(f\"Generated {len(image_embeddings)} image embeddings in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# 2. Generate Embeddings for the Sketch Queries (using validation sketches)\n",
    "start_time = time.time()\n",
    "sketch_embeddings = shared_encoder.predict(val_sketches, batch_size=32)\n",
    "end_time = time.time()\n",
    "print(f\"Generated {len(sketch_embeddings)} sketch embeddings in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Ensure embeddings are numpy arrays for easier manipulation if needed\n",
    "if isinstance(image_embeddings, tf.Tensor):\n",
    "    image_embeddings = image_embeddings.numpy()\n",
    "if isinstance(sketch_embeddings, tf.Tensor):\n",
    "    sketch_embeddings = sketch_embeddings.numpy()\n",
    "\n",
    "# --- Hit Rate Analysis ---\n",
    "\n",
    "print(\"\\n--- Performing Retrieval Analysis ---\")\n",
    "\n",
    "def calculate_hit_rate(sketch_embeddings, image_embeddings, k_values):\n",
    "    \"\"\"\n",
    "    Calculates Hit@K for given K values.\n",
    "    Assumes sketch_embeddings[i] corresponds to image_embeddings[i].\n",
    "    \"\"\"\n",
    "    num_queries = sketch_embeddings.shape[0]\n",
    "    num_images = image_embeddings.shape[0]\n",
    "    hits = {k: 0 for k in k_values}\n",
    "    max_k = max(k_values)\n",
    "\n",
    "    # Calculate all pairwise cosine similarities (dot product since embeddings are L2 normalized)\n",
    "    # Shape: (num_queries, num_images)\n",
    "    start_time = time.time()\n",
    "    similarity_matrix = np.dot(sketch_embeddings, image_embeddings.T)\n",
    "    end_time = time.time()\n",
    "    print(f\"Calculated similarity matrix ({num_queries}x{num_images}) in {end_time - start_time:.2f} seconds.\")\n",
    "    partition_indices = np.argpartition(-similarity_matrix, kth=max_k-1, axis=1)[:, :max_k]\n",
    "    # Now, sort only the top K indices based on their actual similarity values\n",
    "    top_k_indices = np.array([\n",
    "        p_indices[np.argsort(-similarity_matrix[i, p_indices])]\n",
    "        for i, p_indices in enumerate(partition_indices)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Evaluate hits for each query\n",
    "    for i in range(num_queries):\n",
    "        # The ground truth image index is 'i'\n",
    "        ground_truth_index = i\n",
    "        retrieved_indices = top_k_indices[i]\n",
    "\n",
    "        # Check if the ground truth is within the top K results for each K\n",
    "        for k in k_values:\n",
    "            if ground_truth_index in retrieved_indices[:k]:\n",
    "                hits[k] += 1\n",
    "\n",
    "    # Calculate final hit rates\n",
    "    hit_rates = {k: (hits[k] / num_queries) * 100 for k in k_values}\n",
    "    return hit_rates\n",
    "\n",
    "# Define the K values for Hit Rate calculation\n",
    "k_values_to_check = [1, 5, 10]\n",
    "\n",
    "# Calculate the hit rates\n",
    "hit_rates = calculate_hit_rate(sketch_embeddings, image_embeddings, k_values_to_check)\n",
    "\n",
    "print(\"\\n--- Retrieval Performance ---\")\n",
    "for k, rate in hit_rates.items():\n",
    "    print(f\"Hit@{k}: {rate:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Example Retrieval for First Sketch ---\")\n",
    "# Retrieve top 10 images for the first sketch query\n",
    "query_index = 0\n",
    "query_sketch_embedding = sketch_embeddings[query_index:query_index+1] # Keep dimensions\n",
    "\n",
    "# Calculate similarities for this single query\n",
    "similarities = np.dot(query_sketch_embedding, image_embeddings.T)[0] # Result shape (1, num_images), take first row\n",
    "\n",
    "# Get top 10 indices\n",
    "top_10_indices = np.argsort(-similarities)[:10]\n",
    "\n",
    "print(f\"Query Sketch Index: {query_index}\")\n",
    "print(f\"Ground Truth Image Index: {query_index}\")\n",
    "print(f\"Top 10 Retrieved Image Indices: {top_10_indices}\")\n",
    "print(f\"Similarities of Top 10: {similarities[top_10_indices]}\")\n",
    "# Check if the ground truth was retrieved in the top 10\n",
    "if query_index in top_10_indices:\n",
    "    position = np.where(top_10_indices == query_index)[0][0] + 1\n",
    "    print(f\"Correct image found at position {position}.\")\n",
    "else:\n",
    "    print(\"Correct image not found in the top 10.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
